---
title: "Bayesian GLM Part1"
author: "Murray Logan/Can"
date: 30/07
date-format: "30/07/2024Y"
format: 
  html:
    ## Format
    ## Table of contents
    ## Numbering
    ## Layout
    ## Code
    ## Execution
    ## Rendering
    theme: [default, ../resources/ws-style.scss]
    css: ../resources/ws_style.css
    html-math-method: mathjax
    toc: true
    toc-float: true
    number-sections: true
    number-depth: 3
    page-layout: full
    fig-caption-location: "bottom"
    fig-align: "center"
    fig-width: 4
    fig-height: 4
    fig-dpi: 72
    tbl-cap-location: top
    code-fold: false
    code-tools: true
    code-summary: "Show the code"
    code-line-numbers: true
    code-block-border-left: "#ccc"
    code-copy: true
    highlight-style: atom-one
    execute:
      echo: true
      cache: true
    embed-resources: true
crossref:
  fig-title: '**Figure**'
  fig-labels: arabic
  tbl-title: '**Table**'
  tbl-labels: arabic
engine: knitr
output_dir: "docs"
documentclass: article
fontsize: 12pt
mainfont: Arial
mathfont: LiberationMono
monofont: DejaVu Sans Mono
classoption: a4paper
bibliography: ../resources/references.bib
editor: 
  markdown: 
    wrap: 72
---

```{r}
#The chunk of information that begings with "---" at line1 and below is information of preferences that you have on your code chunk like font, size and stuff.

# **WORD** becomes bold if you place it between two **
```

```{r}
#| label: setup
#| include: false

knitr::opts_chunk$set(cache.lazy = FALSE,
                      tidy = "styler")
options(tinytex.engine = "xelatex")
```

# Preparations

Load the necessary libraries

```{r}
#| label: libraries
#| output: false
#| eval: true
#| warning: false
#| message: false
#| cache: false

library(tidyverse)     #for data wrangling etc
library(rstanarm)      #for fitting models in STAN
library(cmdstanr)      #for cmdstan
library(brms)          #for fitting models in STAN
library(standist)      #for exploring distributions
library(coda)          #for diagnostics
library(bayesplot)     #for diagnostics
library(ggmcmc)        #for MCMC diagnostics
library(DHARMa)        #for residual diagnostics
library(rstan)         #for interfacing with STAN
library(emmeans)       #for marginal means etc
library(broom)         #for tidying outputs
library(tidybayes)     #for more tidying outputs
library(HDInterval)    #for HPD intervals
library(ggeffects)     #for partial plots
library(broom.mixed)   #for summarising models
library(posterior)     #for posterior draws
library(ggeffects)     #for partial effects plots
library(patchwork)     #for multi-panel figures
library(bayestestR)    #for ROPE
library(see)           #for some plots
library(easystats)     #framework for stats, modelling and visualisation
#library(INLA)          #for approximate Bayes
#library(INLAutils)     #for additional INLA outputs
#library(modelsummary)  #for data and model summaries 
theme_set(theme_grey()) #put the default ggplot theme back
source('helperFunctions.R')
```

#Packages in bglm_example1

```{r}
# tidyverse -> data wrangling.
# stanarm -> for fitting models into STAN, its a shortcut so it wont be able to fit all models.
# brms ->translates R syntax on STAN syntax.
# standist -> draws distributions for you.
# coda -> very old package to draw distributions for Bayesian statistics, important Bayesian package
# bayesplot -> sits on top of coda, helps you plot.
# ggmc -> ignore it, its a broken package similar to bayesplot.
# DHARMa -> way of diagnosing how to fit a data into a model.
# rstan -> another way of interacting with STAN, also provides diagnostics for your MCMC sampling, a bit nice to Murray's finding.
# emmeans -> fantastic for prediction, testing different hypothesis AFTER you fit a model. Great package for all sort of posthoc.
# broom -> great for tidying data, it works together with tidybayes
# tidybayes -> it provides summary for neat tables you can use to present Bayesian.
# HDInterval -> better way to get credibility (confidence) intervals.
# ggeffects -> provides quick plots
# broom.mixed -> extends broom to account for more models.
# posterior -> when dealing with Bayesian everything is about posteriors, when you have a model it will pull the posteriors out for you.
# patchwork -> allows you to put multiple figures into one and does a great job in that.
# bayestestR -> we use it for ROPE, we will get to that later.
# see -> see the data instead of plotting the data
# easystats -> collection of packages.
# modelsummary -> part of easy stats you dont need to have it if you have easystats.
#source -> loads another R script, this script has functions that Murray find helpfull and that don;t exist otherwhere, he wrote them himself.
```

# Scenario

Here is an example from @Fowler-1998-1998. An agriculturalist was
interested in the effects of fertilizer load on the yield of grass.
Grass seed was sown uniformly over an area and different quantities of
commercial fertilizer were applied to each of ten 1 m<sup>2</sup>
randomly located plots. Two months later the grass from each plot was
harvested, dried and weighed. The data are in the file
**fertilizer.csv** in the **data** folder.

![Field of grass](../resources/turf.jpg){#fig-fertilizer width="70%"}

::: columns
::: {.column width="50%"}
| FERTILIZER | YIELD |
|------------|-------|
| 25         | 84    |
| 50         | 80    |
| 75         | 90    |
| 100        | 154   |
| 125        | 148   |
| ...        | ...   |

: Format of the fertilizer.csv data file {#tbl-fertilizer
.table-condensed}
:::

::: {.column width="50%"}
|                 |                                                   |
|:----------------|:-------------------------------------------------:|
| **FERTILIZER**: | Mass of fertilizer (g.m^-2^) - Predictor variable |
| **YIELD**:      |   Yield of grass (g.m^-2^) - Response variable    |

: Description of the variables in the fertilizer data file
{#tbl-fertilizer1 .table-condensed}
:::
:::

The aim of the analysis is to investigate the relationship between
fertilizer concentration and grass yield.

# Read in the data

We will start off by reading in the Fertilizer data. There are many
functions in R that can read in a CSV file. We will use a the
`read_csv()` function as it is part of the tidyverse ecosystem.

```{r}
#| label: readData
fert <- read_csv("../data/fertilizer.csv", trim_ws = TRUE)
```

::: panel-tabset
#Fertilizer Part

```{r}
# We want to examin is there and effect of fertilizer on yield?
# Is there an optimal amount of fertilizer?
# Is it cost effective?
# What is the rate of change?

# We are gonna think about the best model. y = B0 + BiX

# Gaussian fits the data, it doesnt come close to 0 . The Assumptions of Gaussian are:
# 1- Homogeneity of variance (Residuals are similarly distant)
# 2- Normality
# 3- Linearity 
# 4- Independence (Do you sample randomly)
```

## glimpse

```{r}
#| label: examineData
glimpse(fert)
```

## head

```{r}
## Explore the first 6 rows of the data
head(fert)
```

#Tibble

```{r}
# A matrix has same type of data through out, a data set you can have all sorts of data types. A tibble will promise fit the output on your screen. It wont try and print a million rows for example. It is a more modern approach. In tibble you can nest a dataset in a cell. it allows you to store a dataset of information. You can have each cell, and instead of having to go through multiple loops to plot graphs one after the other, you can just plot graphs with tibble that includes different cells that store data.
```

## str

```{r}
str(fert)
```

## Easystats (datawizard)

```{r}
library(knitr)
fert |> datawizard::data_codebook() |> knitr::kable()
```

## datawizard

```{r}
# ID | Name       | Type    | Missings |    Values |  N
# ---+------------+---------+----------+-----------+---
# 1  | FERTILIZER | numeric | 0 (0.0%) | [25, 250] | 10
# ---+------------+---------+----------+-----------+---
# 2  | YIELD      | numeric | 0 (0.0%) | [80, 248] | 10
# -----------------------------------------------------
```

## Skim (modelsummary)

```{r}
fert |> modelsummary::datasummary_skim()
```
:::

# Exploratory data analysis

Model formula:

$$
\begin{align}
y_i &\sim{} \mathcal{N}(\mu_i, \sigma^2)\\
\mu_i &= \beta_0 + \beta_1 x_i\\
\beta_0 &\sim{} \mathcal{N}(164,65)\\
\beta_1 &\sim{} \mathcal{N}(0,1)\\
\sigma &\sim{} \mathcal{t}(3,0,65)\\
OR\\
\sigma &\sim{} \mathcal{cauchy}(0,65)\\
OR\\
\sigma &\sim{} \mathcal{Exp}(0.016)\\
OR\\
\sigma &\sim{} \mathcal{gamma}(2,0.05)\\
\end{align}
$$

#Weakly Prior

```{r}
#Everything that we need to estimate needs a prior. Intercept and slope needs prior. Since we don't have any priors, we will impose "weakly informative priors" Govern "the bounds of where the drone (sampler) can go. So we don't lose our sampling drone in a proverbial bush on the other side of river.

#We needs priors to keep the samplers in line, they are not used to drive the outcome. Every prior is gonna be a distribution so intercept is gonna follow normal distribution and middle of that is gonna be equal to middle of our data and variance of it is gonna be similar to variance of our data.

#We pick a dot on the slope
```

## SECTION 5 - Fit the Model

# Section 5 - Fit the model

Note, for routines that take more than a couple of seconds to perform
(such as most Bayesian models), it is a good idea to cache the Rmarkdown
chunks in which the routine is performed. That way, the routine will
only be run the first time and any objects generated will be stored for
future use. Thereafter, provided the code has not changed, the routine
will not be re-run. Rather, `knitr` will just retrieve the cached
objects and continue on.

One of the most difficult aspects of performing Bayesian analyses is the
specification of priors. For instances where there are some previous
knowledge available and a desire to incorporate those data, the
difficulty is in how to ensure that the information is incorporated
correctly. However, for instances where there are no previous relevant
information and so a desire to have the posteriors driven entirely by
the new data, the difficulty is in how to define priors that are both
vague enough (not bias results in their direction) and yet not so vague
as to allow the MCMC sampler to drift off into unsupported regions (and
thus get stuck and yield spurious estimates).

For early implementations of MCMC sampling routines (such as Metropolis
Hasting and Gibbs), it was fairly common to see very vague priors being
defined. For example, the priors on effects, were typically normal
priors with mean of 0 and variance of `1e+06` (1,00,000). These are very
vague priors. Yet for some samplers (e.g. NUTS), such vague priors can
encourage poor behaviour of the sampler - particularly if the posterior
is complex. It is now generally advised that priors should (where
possible) be somewhat **weakly informative** and to some extent,
represent the bounds of what are feasible and sensible estimates.

The degree to which priors **influence** an outcome (whether by having a
pulling effect on the estimates or by encouraging the sampler to drift
off into unsupported regions of the posterior) is dependent on:

-   the relative sparsity of the data - the larger the data, the less
    weight the priors have and thus less influence they exert.
-   the complexity of the model (and thus posterior) - the more
    parameters, the more sensitive the sampler is to the priors.

The sampled posterior is the product of both the likelihood and the
prior - all of which are multidimensional. For most applications, it
would be vertically impossible to define a sensible multidimensional
prior. Hence, our only option is to define priors on individual
parameters (e.g. the intercept, slope(s), variance etc) and to hope that
if they are individually sensible, they will remain collectively
sensible.

So having (hopefully) impressed upon the notion that priors are an
important consideration, I will now attempt to synthesise some of the
approaches that can be employed to arrive at weakly informative priors
that have been gleaned from various sources. Largely, this advice has
come from the following resources:

-   https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations
-   http://svmiller.com/blog/2021/02/thinking-about-your-priors-bayesian-analysis/

I will outline some of the current main recommendations before
summarising some approaches in a table.

-   weakly informative priors should contain enough information so as to
    regularise (discourage unreasonable parameter estimates whilst
    allowing all reasonable estimates).
-   for effects parameters on scaled data, an argument could be made for
    a normal distribution with a standard deviation of 1 (e.g.
    `normal(0,1)`), although klsome prefer a t distribution with 3
    degrees of freedom and standard deviation of 1 (e.g.
    `student_t(3,0,1)`) - apparently a flatter t is a more robust prior
    than a normal as an uninformative prior...
-   for unscaled data, the above priors can be scaled by using the
    standard deviation of the data as the prior standard deviation (e.g.
    `student_t(3,0,sd(y))`, or `sudent_t(3,0,sd(y)/sd(x))`)
-   for priors of hierachical standard deviations, priors should
    encourage shrinkage towards 0 (particularly if the number of groups
    is small, since otherwise, the sampler will tend to be more
    responsive to "noise").

| Family            | Parameter                            | brms                            | rstanarm                    |
|------------------|-------------------|------------------|------------------|
| Gaussian          | Intercept                            | `student_t(3,median(y),mad(y))` | `normal(mean(y),2.5*sd(y))` |
|                   | 'Population effects' (slopes, betas) | flat, improper priors           | `normal(0,2.5*sd(y)/sd(x))` |
|                   | Sigma                                | `student_t(3,0,mad(y))`         | `exponential(1/sd(y))`      |
|                   | 'Group-level effects'                | `student_t(3,0,mad(y))`         | `decov(1,1,1,1)`            |
|                   | Correlation on group-level effects   | `ljk_corr_cholesky(1)`          |                             |
| Poisson           | Intercept                            | `student_t(3,median(y),mad(y))` | `normal(mean(y),2.5*sd(y))` |
|                   | 'Population effects' (slopes, betas) | flat, improper priors           | `normal(0,2.5*sd(y)/sd(x))` |
|                   | 'Group-level effects'                | `student_t(3,0,mad(y))`         | `decov(1,1,1,1)`            |
|                   | Correlation on group-level effects   | `ljk_corr_cholesky(1)`          |                             |
| Negative binomial | Intercept                            | `student_t(3,median(y),mad(y))` | `normal(mean(y),2.5*sd(y))` |
|                   | 'Population effects' (slopes, betas) | flat, improper priors           | `normal(0,2.5*sd(y)/sd(x))` |
|                   | Shape                                | `gamma(0.01, 0.01)`             | `exponential(1/sd(y))`      |
|                   | 'Group-level effects'                | `student_t(3,0,mad(y))`         | `decov(1,1,1,1)`            |
|                   | Correlation on group-level effects   | `ljk_corr_cholesky(1)`          |                             |

```{r}
# |label: EDA3
# |message: false
ggplot(fert, aes(y = YIELD)) +
  geom_boxplot() 
ggplot(fert, aes(y = YIELD, x = FERTILIZER)) +
  geom_point() +
  geom_smooth(method = "lm")

```

```{r}
# We first check normality of distribution of data with boxplot. You might also do it with violin plot but you need quite bit of data for that.

#str function will give entire aspects of object. what are the attributes are as well.
```

##ggplot

```{r}
# ggplot -> grammer of graphics. 
# aes -> stands for aesthetics, mapping of the plot.
# geom -> what geometric shape you use to represent your data.

## ideally you want your boxplot to be symmetrical.
## you can use transformations for predictors (like the cause Ex; fertilizer is predictor, yield is result) we dont seek normality in predictors. it is on the people that does the experiment.

#geom_smooth -> adds a smoother that suggests if there is any major changes in direction and helps you identify non-linearity.
# you can add smoother using a simple linear model with geom_smooth(method "lm"). This also suggests homogeneity because we can observe pattern in residuals by how the dots allign around the line.
```

```{r}
ggplot(fert, aes(y = YIELD)) +
  geom_boxplot(aes(x = 1)) 
ggplot(fert, aes(y = YIELD, x = FERTILIZER)) +
  geom_point()
ggplot(fert, aes(y = YIELD, x = FERTILIZER)) +
  geom_point() +
  geom_smooth()
ggplot(fert, aes(y = YIELD, x = FERTILIZER)) +
  geom_point() +
  geom_smooth(method = "lm")  
median(fert$YIELD)
mad(fert$YIELD) # variance
mad(fert$YIELD)/mad(fert$FERTILIZER)

```

```{r}
# median is the intercept.
# We always set middle to be 0, we dont want to influence the data to skew by having mean negative or positive. Mean of the slope is 0 and variance of the slope is 1 ??? -> annoatation -> B1 ~ N(0,1)
# Slope -> change in ratio of y to change of x -> quit
# mad gives variance, so one of the assumptions of Gaussian is variance is same across.
# to find sigma we will use t-distribution. and we set middle 0. A typical distribution of variance is half of a normal distribtuion, it is cut in half because we cant have negative standard deviation.
# For priors you dont need to have exact numbers, you can round them.

# Summary of annotations:
# y = b0 + b1 * x
# b0 = ~ N(161,90)
# b1 ~ N(0,1)
# S^2 = ~ t(3,0,90) -> 3 is best, no explanation.
```

```{r}
# Our priors averaged at 161, Our slope is 0 which was set by priors
```

#Centering your predictors

```{r}
# If you substract mean from all the values on x-axis, the middle of the distribution will be 0. which is called "Centering"
# Slope is unaffected by centering. 
# Intercept will move, meaning it has no meaning at all.
# We dont need the part outside of our data, once we center the predictors, intercept will correspond to outer skirts of our data which we are not interested in.
# You should always center your predictors for Bayesian analysis.
```

#Scaling your predictors

```{r}
# Scaling your predictors will affect the slope. 
# It will make it easier to determine priors.
# Once you center the data, all the predictors will be themselves because the variance of the predictor will become 1.
# 
```

## Start 1 defining your formula

```{r}
# creating a formula called "form"
form <- bf(YIELD ~ FERTILIZER) #bf is for bayesian formula, YIELD is our result, the line reads as "YIELD PROPORTIONAL TO FERTILIZER) This assumes you want an intercept
#form <- bf(YIELD ~ 1 + FERTILIZER) # 1 significies intercept, we want intercept + slope.
# every term to right hand side will gets a parameter.
#form <- bf(YIELD ~ 0 + FERTILIZER) #this indicates you don't want intercept but it is so rare. you dont see it.
priors <- prior(normal(161,90), class = "Intercept") +
prior(normal(0,1), class = "b") + # b is for slope that is how its indicated
prior(student_t(3,0,90), class = "sigma")

fert.brm1 <- brm(form,
  data = fert,
  prior = priors,
  sample_prior = 'only', #sample only from priors, if you change "only" to "yes" it will also sample from posterior
  iter = 5000, # sets how many links in the chains. how many steps that the chain will include. 
  warmup = 1000, # before running the chain, program has to learn how to run, like pushing the ball from the mountain from 29/07. this line ignore the first 1000 runs, which will make the program learn how to do it, the 4000 (iter = 5000) after the first 1000 will be included
  chains = 3, cores =3, # we are going to repeat this 3 times, if we get the same posteriors 3 times from 3 different chains, we can be more confident of how the priors represented our data. You typically ask for 2,3 chains. "cores" will incorporated different cores of the computer so they will run simultaneously in different cores of PC.
  thin = 5, # only keeps every 5th value which prevents the values to correlate with one another. We needed 1000 samples minimum, we have 4000x3 = 12000/5 = 2400, enough amount of sampling.
  backend = 'cmdstanr',
  refresh = 0) # 
```

#Plotting the model

```{r}
#These priors are based on the data we already had and seem to be appropriate. There is a lot of flexibility and the bounds of the sampler are reasonable 
#This model only shows the first species (if you have more than one, this might be a problem)
fert.brm1 |> 
  ggpredict() |> 
  plot(show_data = TRUE)

#This yields the same thing as the above, however, it is marginalising over your categories, So if you have data for more than one species of grass you will get an estimate for both species.
fert.brm1 |> 
  ggemmeans(~FERTILIZER) |> 
  plot(show_data = TRUE)

#This will do every model but it wont always br pretty, we are going to use this for the course because we know that it works for basically everything.
fert.brm1 |> 
  conditional_effects() |> 
  plot(points = TRUE)
```

```{r}
fert.brm3 <- update(fert.brm1, sample_prior = 'yes', refresh = 0) #take an existing model and update the things we specify
```

#Save the data

```{r}
saveRDS(fert.brm3, file = 'fert.brm3.RData') #it is good practice to save the model, so you dont have to run model again and take time with it.
save(fert.brm3, form, fert, priors, file = "fert.model3.RData") #This saves everything within
fert.brm3 <- readRDS(file = "fert.brm3.RData")
load(file = "fert.model3.RData")
```

```{r}
fert.brm3 |> 
conditional_effects() |> 
  plot(points = TRUE)
```

```{r}
#| label: Priors
fert.brm3 |>  SUYR_prior_and_posterior()

# The black one is "prior" in each panel. We want them to be wide, means we have wide range of possibility of priors, not opiniated.
# If the priors are influential, posteriors will look like the prior.
# In this plot, the outcomes do not look like priors therefore the data is influential on the posterior. Meaning we need to have a wider range of prior.
```

```{r}
#| label: Correlation
fert.brm3$fit |>  stan_trace() # You can keep editing this way
fert.brm3$fit |>  stan_ac() # We are not concerned with priors with this one? Lag0 has high correlation because it is correlation with it self. You dont wanna see high correlation besides that because we thinned the sampling so that correlation would not occur, samples will remain independent.

#The spikes mean sampler drifted a bit but its okay, they came back
```

#Rhat

```{r}
fert.brm3$fit |>  stan_rhat()
#Rhat is a matrix of convergence, Rhat value chains and how well converged they are. You want to make sure all Rhat values are less than 1.01.
```

```{r}
# It is effective the values are more than 0.5. If not the data is not very reliable. It can suggest your effective sample size is a bit low. You can try decrease your range of prior.
fert.brm3$fit |> stan_ess()
```

```{r}
#if you want to see what each posterior distributions look like we can plot them. This example plots chains seperately.
fert.brm3$fit |>  stan_dens(separate_chains = TRUE)
# high convergence is determined by how they look on top of each other. In this one it is almost perfectly on top of each other. If they dont allign, that means you need to sample for longer so that they will cover more areas and the overlap would be higher therefore higher convergence.
```

Notes:

`brms`

https://github.com/paul-buerkner/brms/blob/c2b24475d727c8afd8bfc95947c18793b8ce2892/R/priors.R

1.  In the above, for non-Gaussian families, `y` is first transformed
    according to the family link. If the family link is `log`, then 0.1
    is first added to 0 values.
2.  in `brms` the minimum standard deviation for the Intercept prior is
    `2.5`
3.  in `brms` the minimum standard deviation for group-level priors is
    `10`.

`rstanarm`

http://mc-stan.org/rstanarm/articles/priors.html

1.  in `rstanarm` priors on standard deviation and correlation
    associated with group-level effects are packaged up into a single
    prior (`decov` which is a decomposition of the variance and
    covariance matrix).

# Section 6 - MCMC sampling diagnostics

**MCMC sampling behaviour**

Since the purpose of the MCMC sampling is to estimate the posterior of
an unknown joint likelihood, it is important that we explore a range of
diagnostics designed to help identify when the resulting likelihood
might not be accurate.

-   **traceplots** - plots of the individual draws in sequence. Traces
    that resemble noise suggest that all likelihood features are likely
    to have be traversed. Obvious steps or blocks of noise are likely to
    represent distinct features and could imply that there are yet other
    features that have not yet been traversed - necessitating additional
    iterations. Furthermore, each chain should be indistinguishable from
    the others
-   **autocorrelation function** - plots of the degree of correlation
    between pairs of draws for a range of lags (distance along the
    chains). High levels of correlation (after a lag of 0, which is
    correlating each draw with itself) suggests a lack of independence
    between the draws and that therefore, summaries such as mean and
    median will be biased estimates. Ideally, all non-zero lag
    correlations should be less than 0.2.
-   **convergence diagnostics** - there are a range of diagnostics aimed
    at exploring whether the multiple chains are likely to have
    converged upon similar posteriors
    -   **R hat** - this metric compares between and within chain model
        parameter estimates, with the expectation that if the chains
        have converged, the between and within rank normalised estimates
        should be very similar (and Rhat should be close to 1). The more
        one chains deviates from the others, the higher the Rhat value.
        Values less than 1.05 are considered evidence of convergence.
    -   **Bulk ESS** - this is a measure of the effective sample size
        from the whole (bulk) of the posterior and is a good measure of
        the sampling efficiency of draws across the entire posterior
    -   **Tail ESS** - this is a measure of the effective sample size
        from the 5% and 95% quantiles (tails) of the posterior and is a
        good measure of the sampling efficiency of draws from the tail
        (areas of the posterior with least support and where samplers
        can get stuck).

`available_mcmc()`

| Package   | Description       | function               | rstanarm                         | brms                               |
|---------------|---------------|---------------|---------------|---------------|
| bayesplot | Traceplot         | `mcmc_trace`           | `plot(mod, plotfun='trace')`     | `mcmc_plot(mod, type='trace')`     |
|           | Density plot      | `mcmc_dens`            | `plot(mod, plotfun='dens')`      | `mcmc_plot(mod, type='dens')`      |
|           | Density & Trace   | `mcmc_combo`           | `plot(mod, plotfun='combo')`     | `mcmc_plot(mod, type='combo')`     |
|           | ACF               | `mcmc_acf_bar`         | `plot(mod, plotfun='acf_bar')`   | `mcmc_plot(mod, type='acf_bar')`   |
|           | Rhat hist         | `mcmc_rhat_hist`       | `plot(mod, plotfun='rhat_hist')` | `mcmc_plot(mod, type='rhat_hist')` |
|           | No. Effective     | `mcmc_neff_hist`       | `plot(mod, plotfun='neff_hist')` | `mcmc_plot(mod, type='neff_hist')` |
| rstan     | Traceplot         | `stan_trace`           | `stan_trace(mod)`                | `stan_trace(mod)`                  |
|           | ACF               | `stan_ac`              | `stan_ac(mod)`                   | `stan_ac(mod)`                     |
|           | Rhat              | `stan_rhat`            | `stan_rhat(mod)`                 | `stan_rhat(mod)`                   |
|           | No. Effective     | `stan_ess`             | `stan_ess(mod)`                  | `stan_ess(mod)`                    |
|           | Density plot      | `stan_dens`            | `stan_dens(mod)`                 | `stan_dens(mod)`                   |
| ggmcmc    | Traceplot         | `ggs_traceplot`        | `ggs_traceplot(ggs(mod))`        | `ggs_traceplot(ggs(mod))`          |
|           | ACF               | `ggs_autocorrelation`  | `ggs_autocorrelation(ggs(mod))`  | `ggs_autocorrelation(ggs(mod))`    |
|           | Rhat              | `ggs_Rhat`             | `ggs_Rhat(ggs(mod))`             | `ggs_Rhat(ggs(mod))`               |
|           | No. Effective     | `ggs_effective`        | `ggs_effective(ggs(mod))`        | `ggs_effective(ggs(mod))`          |
|           | Cross correlation | `ggs_crosscorrelation` | `ggs_crosscorrelation(ggs(mod))` | `ggs_crosscorrelation(ggs(mod))`   |
|           | Scale reduction   | `ggs_grb`              | `ggs_grb(ggs(mod))`              | `ggs_grb(ggs(mod))`                |
|           |                   |                        |                                  |                                    |

## SECTION 7 - Model Validation

**Posterior probability checks**

`available_ppc()`

| Package   | Description       | function                     | rstanarm                                              | brms                                               |
|---------------|---------------|---------------|---------------|---------------|
| bayesplot | Density overlay   | `ppc_dens_overlay`           | `pp_check(mod, plotfun='dens_overlay')`               | `pp_check(mod, type='dens_overlay')`               |
|           | Obs vs Pred error | `ppc_error_scatter_avg`      | `pp_check(mod, plotfun='error_scatter_avg')`          | `pp_check(mod, type='error_scatter_avg')`          |
|           | Pred error vs x   | `ppc_error_scatter_avg_vs_x` | `pp_check(mod, x=, plotfun='error_scatter_avg_vs_x')` | `pp_check(mod, x=, type='error_scatter_avg_vs_x')` |
|           | Preds vs x        | `ppc_intervals`              | `pp_check(mod, x=, plotfun='intervals')`              | `pp_check(mod, x=, type='intervals')`              |
|           | Partial plot      | `ppc_ribbon`                 | `pp_check(mod, x=, plotfun='ribbon')`                 | `pp_check(mod, x=, type='ribbon')`                 |
|           |                   |                              |                                                       |                                                    |

```{r}
fert.brm3 |>  pp_check(type ='dens_overlay', ndraws=100)

#Black line is density distribution of your data.
#Light blue lines are 100 predicted distributions of your data. You can plot up to 2500.
```

```{r}
fert.brm3 |>  pp_check(type = 'error_scatter_avg')
# You dont want to see a pattern in a scatterplot. It is not a great residual plot.
```

#Residual checks

```{r}
fert.resids <- make_brms_dharma_res(fert.brm3, integerResponse = FALSE)
testUniformity(fert.resids)

# Always interpret the same way, DHARMa. DHARMa residuals create simulate residuals that then you can compare to your actual residuals.
# QQplot will plot your data or quantiles of your data and compares them to how you want your data to look like, as a distribution. if they follow the line they match the normal distrbution.
# Simulated residuals are made from the model that was made from your data. You take your data and predict from it hundreds of times and calculate residuals. Simulated ones.
```

```{r}
plotResiduals(fert.resids)

#To hope to identify any patterns, lines separate thirds of your data (25, 50 ,75)
```

# Partial effects plots

# Model investigation

```{r}
#Almost all statistical analysis have a summary method.

# Estimate -> peak of MEAN of distribution.
# Rhat -> measure of convergence, we want them less than 1.01
# Bulk_ESS -> middle part of distribution, how effective it was to collect from the middle of distribition, you want these to be above 1000.
# Tail_ESS -> similar to bulk but refers to tail ends of distribution.

fert.brm3 |> summary()

#priors should not include 0 in their credible interval range (confidence). if the values goes from negative to positive or visa versa, they include 0 so they are reliable. Estimate of prior is slope. but it could be any other number between CI (Credible Interval) 

# We did not center our data so this indicates 52.59 intercept meaning if we don't use any fertilizer we will get yield of 52.59. However we are extrapolating something we actually didnt do.
```

```{r}
# Better way to summarize according to Murray
fert.brm3 |>  as_draws_df() |> 
  summarise_draws(median)
```

# 

```{r}

#medians are better 
fert.brm3 |>  as_draws_df() |> 
  summarise_draws(median, HDInterval::hdi) #highest probability density intervals. hdi take it into account of how further away the quantiles are, where as without hdi it only considers rank)

fert.brm3 |>  as_draws_df() |> 
  summarise_draws(median, HDInterval::hdi, rhat, length, ess_bulk, ess_tail)
```

```{r}
# Checking how confident I am that there is an effect? We can calculate that probability, we can calculate the probability of slope is greater than 0? the prediction was 0.80 so we are checking the positive ones because we strongly believe that effect is positive. 

fert.brm3 |>  as_draws_df() |> 
 summarise_draws(median, HDInterval::hdi, rhat, length, ess_bulk, ess_tail, Pg = ~mean(.x > 0))

# We can calculate the mean of scenarios were effect is positive (1) and negative (0), that mean will give us the probability of fertilizer effect is positive on yield. We know that the prediction was 0.8 so slope being larger than 0 is most highly likely.
```

```{r}
#We dont want the information comes after sigma
fert.brm3 |>  as_draws_df() #b_Intercept is the one thats not centered, Intercept column gives centered data

fert.brm3 |>  as_draws_df() |> 
 dplyr::select(starts_with("b_"), "sigma")

#scale function is also centers the data, Ex: scale(1:10, scale = FALSE) doing centering in the formula with scale helps with storing the mean as well so you can back-transform if needed.
# if you get rid of "FALSE" -> scale(1:10) -> it will center and scale and stores the standard deviation and mean.

scale(1:10)
```

R\^2

```{r}
#R^2 -> how much of the total variability your model was able to explain. It is based on your residuals, 1 - sum your residuals)/(total amount of residuals) thats how it is calculated in "Frequentist" analysis. Residuals are not an integral part of Bayesian analysis. You can call it "Quasi R^2" for Bayesian R^2.

fert.brm3 |> 
  bayes_R2(summary = FALSE) |>  median_hdci()

# The median (0.9188) is much closer to the max value of R2. Median and HDIs reflect this skewness better than  quartiles and mean. Almost 92% of variance in yield (increase in this case) can be explained by fertilizer.
```

Rather than simply return point estimates of each of the model
parameters, Bayesian analyses capture the full posterior of each
parameter. These are typically stored within the `list` structure of the
output object.

As with most statistical routines, the overloaded `summary()` function
provides an overall summary of the model parameters. Typically, the
summaries will include the means / medians along with credibility
intervals and perhaps convergence diagnostics (such as R hat). However,
more thorough investigation and analysis of the parameter posteriors
requires access to the full posteriors.

There is currently a plethora of functions for extracting the full
posteriors from models. In part, this is a reflection of a rapidly
evolving space with numerous packages providing near equivalent
functionality (it should also be noted, that over time, many of the
functions have been deprecated due to inconsistencies in their names).
Broadly speaking, the functions focus on draws from the posterior of
either the parameters (intercept, slope, standard deviation etc),
**linear predictor**, **expected values** or **predicted values**. The
distinction between the latter three are highlighted in the following
table.

| Property          | Description                                                                                  |
|---------------------|---------------------------------------------------|
| linear predictors | values predicted on the link scale                                                           |
| expected values   | predictions (on response scale) without residual error (predicting expected mean outcome(s)) |
| predicted values  | predictions (on response scale) that incorporate residual error                              |
|                   |                                                                                              |
| fitted values     | predictions on the response scale                                                            |

The following table lists the various ways of extracting the full
posteriors of the model parameters parameters, **expected** values and
**predicted** values. The crossed out items are now deprecated and
function with a namespace of `__` mean that the functionality is
provided via a range of packages.

| Function                        | Values            | Description                                                                                                                                                       |
|--------------------|-------------------|---------------------------------|
| `__::as.matrix()`               | Parameters        | Returns $n\times p$ matrix                                                                                                                                        |
| `__::as.data.frame()`           | Parameters        | Returns $n\times p$ data.frame                                                                                                                                    |
| `__::as_tibble()`               | Parameters        | Returns $n\times p$ tibble                                                                                                                                        |
| `posterior::as_draws_df()`      | Parameters        | Returns $n\times p$ data.frame with additional info about chain, interaction and draw                                                                             |
| ~~`brms::posterior_samples()`~~ | ~~Parameters~~    | ~~Returns~~ $n\times p$ data.frame                                                                                                                                |
| `tidybayes::tidy_draws()`       | Parameters        | Returns $n\times p$ tibble with addition info about the chain, iteration and draw                                                                                 |
| `rstan::extract()`              | Parameters        | Returns a $p$ length list of $n$ length vectors                                                                                                                   |
| `tidybayes::spread_draws()`     | Parameters        | Returns $n\times r$ tibble with additional info about chain, interaction and draw                                                                                 |
| `tidybayes::gather_draws()`     | Parameters        | Returns a gathered `spread_draws` tibble with additional info about chain, interaction and draw                                                                   |
| `rstanarm::posterior_linpred()` | Linear predictors | Returns $n\times N$ tibble on the link scale                                                                                                                      |
| `brms::posterior_linpred()`     | Linear predictors | Returns $n\times N$ tibble on the link scale                                                                                                                      |
| `tidybayes::linpred_draws()`    | Linear predictors | Returns tibble with \$n\times N rows and `.linpred` on the link scale additional info about chain, interaction and draw                                           |
| `rstanarm::posterior_epred()`   | Expected values   | Returns $n\times N$ tibble on the response scale                                                                                                                  |
| `brms::posterior_epred()`       | Expected values   | Returns $n\times N$ tibble on the response scale                                                                                                                  |
| `tidybayes::epred_draws()`      | Expected values   | Returns tibble with \$n\times N rows and `.epred` on the response scale additional info about chain, interaction and draw                                         |
| `rstanarm::posterior_predict()` | Expected values   | Returns $n\times N$ tibble of predictions (including residuals) on the response scale                                                                             |
| `brms::posterior_predict()`     | Expected values   | Returns $n\times N$ tibble of predictions (including residuals) on the response scale                                                                             |
| `tidybayes::predicted_draws()`  | Expected values   | [@gelman2003]Returns tibble with \$n\times N rows and `.prediction` (including residuals) on the response scale additional info about chain, interaction and draw |

where $n$ is the number of MCMC samples and $p$ is the number of
parameters to estimate, $N$ is the number of newdata rows and $r$ is the
number of requested parameters. For the `tidybayes` versions in the
table above, the function expects a model to be the first parameter (and
a dataframe to be the second). There are also `add_` versions which
expect a dataframe to be the first argument and the model to be the
second. These alternatives facilitate pipings with different starting
objects.

| Function      | Description                                                                    |
|--------------------|----------------------------------------------------|
| `median_qi`   | Median and quantiles of specific columns                                       |
| `median_hdi`  | Median and Highest Probability Density Interval of specific columns            |
| `median_hdci` | Median and continuous Highest Probability Density Interval of specific columns |
| `tidyMCMC`    | Median/mean and quantiles/hpd of all columns                                   |

# Section 8 - Predictions

```{r}
# You can calculate

newdata <- data.frame(FERTILIZER = 100) # We are not  calling this data from fert dataset, we are creating a new column with exact same name, it has to have exact same name. This is summary, it gives exactly what you want.
```

```{r}
# This is better way of predicting of FERTILIZER concentration that is 100

predict(fert.brm3, newdata = newdata)
epred_draws(fert.brm3, newdata = newdata) # .epred column holds all 2400 predictions.

##
```

# Hypothesis testing

```{r}
newdata <- data.frame(FERTILIZER = c(100,200)) # allows us to utilize more than 1 number to be considered, it equates to 100 first then 200
epred_draws(fert.brm3, newdata = newdata) |> dplyr::select(FERTILIZER, .epred) |> #epred e letter means expected predictions -> always back transformed from log so we are back on the count type not log scale.
  summarise_draws(median, HDInterval::hdi, Pg120 = ~mean(.x  >120))

fert.brm3 |>  # predictions of the yield at 100 to 200 conc of FERTILIZER
  emmeans(~FERTILIZER, at = newdata) # It tells us in the bottom it is using medians and hpd intervals
# this

#These calculation happens with same draws of samples out of 2400. once the draws are finished, being calculated, then it is summarized.

fert.brm3 |>  # it gives us the difference in yield between 200 to 100.
  emmeans(~FERTILIZER, at = newdata) |> 
  pairs()

fert.brm3 |> # it changes the order of the calculation of the difference.
  emmeans(~FERTILIZER, at = newdata) |> 
  pairs(reverse = TRUE)

fert.brm3 |>   
  emmeans(~FERTILIZER, at = newdata) |> 
  pairs(reverse = TRUE) |> 
  gather_emmeans_draws() |> # stop summarizes gives all outcomes from draws that it makes.
  dplyr::select(contrast, .value) |> # gives entire draws and gather them on to one column, you can patch summarize on that.
  summarise_draws(median, HDInterval::hdi,
                  Pg60 = ~mean(.x >60)) # exceedance probability. where we calculate possbility of difference of 100 to 200 be more than 60.
  
```

```{r}
# 3 values are compared within each other.
newdata <- data.frame(FERTILIZER = c(100,150,200))

fert.brm3 |>
  emmeans(~FERTILIZER, at = newdata) |> 
  pairs(reverse = TRUE) |> 
  gather_emmeans_draws() |> 
  dplyr::select(contrast, .value) |> 
  tidybayes::summarise_draws(median, HDInterval::hdi,
                           Pg60 = ~mean(.x >60))
```

```{r}
# We use values that the sampler use to measure the yield in fertilizer axes. the 
newdata <- with(fert, data.frame(FERTILIZER = seq(min(FERTILIZER), max(FERTILIZER), len = 100)))
head(newdata)
tail(newdata)
fert.em <-
fert.brm3 |> 
  emmeans (~FERTILIZER, at = newdata) |> 
  as.data.frame() #graphs need to be dataframes or tibbles

```

# Summary figures

```{r}
#| label: fig-1
#| fig-cap: Grass yield against fertilizer conc
#| fig-width: 6
#| fig-height: 4.5
#| out-width: "500"

save(fert.em, newdata, file = "fert.em.RData")
load(file = "fert.em.RData") 

graph1 <-
fert.em |> 
ggplot(aes(y = emmean, x = FERTILIZER)) +
  geom_ribbon(aes(ymin = lower.HPD, ymax = upper.HPD), fill = 'orange', alpha = 0.4) +
  geom_line(colour = "red") +
  geom_point( data = fert, aes(y = YIELD)) + # We added aes(y = FIELD) to override y = emmean from couple lines above. If we do not, geom_point function will check and inherit emmeans column remains in fert dataset above.
  scale_y_continuous(expression(Grass~yield~(g.m^-2)), breaks = seq(50,300, by = 50)) + # We need expression part for the reason that it includes a mathematical expression in the title. To include -2 as super string (power). Otherwise you can use "" to include string. You can check how to include expression by typing demo(plotmath) on console only. 
  scale_x_continuous(expression(Fertilizer~concentration~(g.m^-2))) +
  theme_classic()

graph1
ggsave(file = "Figure2.pdf", graph1, width = 6, height = 6/1.6)
ggsave(file = "Figure2.png", graph1, width = 6, height = 6/1.6, dpi = 300)
```

# Methods

```{r}
## How to add citations -> go to Visual from Source from upper left corner. The citation cant be in a chunk. then go insert from the tiny menu up. click citations
```

-   The relationship between grass yield and fertilizer concentration
    was explored using linear regression in Bayesian
    framework.[@gelman2003]

-   specifically, the yield of grass was modelled against fertiliser
    concetration with Gaussian family and weakly informative priors, see
    Supp.1)

$$
\begin{align}
y_i &\sim{} \mathcal{N}(\mu_i, \sigma^2)\\
\mu_i &= \beta_0 + \beta_1 x_i\\
\beta_0 &\sim{} \mathcal{N}(164,65)\\
\beta_1 &\sim{} \mathcal{N}(0,1)\\
\sigma &\sim{} \mathcal{t}(3,0,65)\\
OR\\
\sigma &\sim{} \mathcal{cauchy}(0,65)\\
OR\\
\sigma &\sim{} \mathcal{Exp}(0.016)\\
OR\\
\sigma &\sim{} \mathcal{gamma}(2,0.05)\\
\end{align}
$$

- the Bayesian model included three chains, each of 5000 iterations,
thinned to a rate of five, and excluding the first 1000 iterations.
-   the model was found to be well mixed and converged (all Rhat \<
1.01) on a stable posterior (see Supp. 1) and was validated via
simulated residuals using DHARMa package ()[@DHARMa]
- we also performed post-hoc comparisons of the expected yields associated with fertilizer concentratins of 100 and 200 g.m^-2. These comparisons were performed on the full posteriors before being summarised as medians, highest probability density internvals along with specific exceedence probabilities.
- all statistical models were performed in the R [@base](4.4.1)
Statistical and Graphical Environment () via the brms package
()[@brms-2]

# Results
- Grass yield was found to have a positive linear relationship with fertiliser concentration (P_exceed, median, intervals see @fig-1)
- Every one unit increase in fertiliser concentration was found to be associated with a 0.8 gram increase in grass yield (see @fig-1)
- Doubling the fertiliser concentration from 100 to 200 is expected to increase the yield by 80.6% (95% CI) grams
# References
